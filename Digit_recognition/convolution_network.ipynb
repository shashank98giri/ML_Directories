{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.image as imgplt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display,Image\n",
    "from scipy import misc\n",
    "import cntk as C\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "input_dim_model=(1,28,28)\n",
    "num_labels=10\n",
    "input_dim=28*28\n",
    "\n",
    "def create_reader(path,istraining):\n",
    "    deserializer=C.io.CTFDeserializer(path,C.io.StreamDefs(labels=C.io.StreamDef(field=\"labels\",shape=num_labels,is_sparse=False),\n",
    "                                                      features=C.io.StreamDef(field=\"features\",shape=input_dim,is_sparse=False)))\n",
    "    return C.io.MinibatchSource(deserializer,randomize=istraining,max_sweeps=C.io.INFINITELY_REPEAT if istraining else 1)\n",
    "\n",
    "x=C.input_variable(input_dim_model)\n",
    "y=C.input_variable(num_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is data/MNIST\n"
     ]
    }
   ],
   "source": [
    "data_found=False # A flag to indicate if train/test data found in local cache\n",
    "for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    \n",
    "    train_file=os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file=os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    \n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found=True\n",
    "        break\n",
    "        \n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing Lab1_MNIST_DataLoader\")\n",
    "    \n",
    "print(\"Data directory is {0}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "            h = features\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=8, \n",
    "                                       strides=(2,2), \n",
    "                                       pad=True, name='first_conv')(h)\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=16, \n",
    "                                       strides=(2,2), \n",
    "                                       pad=True, name='second_conv')(h)\n",
    "            r = C.layers.Dense(num_labels, activation=None, name='classify')(h)\n",
    "            return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "z=create_model(x/255)\n",
    "\n",
    "def create_criterion_function(model, labels):\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error(model, labels)\n",
    "    return loss,errs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_reader,test_reader,model_func):\n",
    "    model=model_func(x/255)\n",
    "    loss,label_error=create_criterion_function(model,y)\n",
    "    lr_schedule=C.learning_rate_schedule(0.2,C.UnitType.minibatch)\n",
    "    learner=C.sgd(z.parameters,lr_schedule)\n",
    "    trainer=C.Trainer(z,(loss,label_error),[learner])\n",
    "    \n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_sweeps_to_train_with=10\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    input_map={\n",
    "        y  : train_reader.streams.labels,\n",
    "        x  : train_reader.streams.features\n",
    "    } \n",
    "    plot_data={\"batchsize\":[],\"loss\":[],\"error\":[]}\n",
    "    training_progress_output_freq=500\n",
    "    start=time.time()\n",
    "    for i in range (int(num_minibatches_to_train)):\n",
    "        loss=\"NA\"\n",
    "        error=\"NA\"\n",
    "        data=train_reader.next_minibatch(minibatch_size,input_map=input_map)\n",
    "        trainer.train_minibatch(data)\n",
    "        batchsize,loss,eval_error=print_training_progress(trainer,i,training_progress_output_freq,verbose=1)\n",
    "        if not (loss == \"NA\" or error ==\"NA\"):\n",
    "            \n",
    "            plot_data[\"batchsize\"].append(batchsize)\n",
    "            plot_data[\"loss\"].append(loss)\n",
    "            plot_data[\"error\"].append(error)\n",
    "    print(\"Training took {:.1f} s\".format(time.time()-start))\n",
    " \n",
    "\n",
    "    #    plot_data[\"avg_loss\"]=moving_average(plot_data[\"loss\"])\n",
    "#     plot_data[\"avg_error\"]=moving_average(plot_data[\"error\"])\n",
    "#     plt.figure(1)\n",
    "#     plt.subplot(2,1,1)\n",
    "#     plt.plot(plot_data[\"batchsize\"],plot_data[\"avg_loss\"],\"b--\")\n",
    "#     plt.xlabel(\"batchsize\")\n",
    "#     plt.ylabel(\"loss\")\n",
    "#     plt.show()\n",
    "#     plt.subplot(2,1,2)\n",
    "#     plt.plot(plot_data[\"batchsize\"],plot_data[\"avg_error\"],\"r--\")\n",
    "#     plt.xlabel(\"batchsize\")\n",
    "#     plt.ylabel(\"error\")\n",
    "#     plt.show()\n",
    "\n",
    "        \n",
    "    test_input_map = {\n",
    "        y  : test_reader.streams.labels,\n",
    "        x  : test_reader.streams.features\n",
    "        }\n",
    "\n",
    "   \n",
    "    test_minibatch_size = 512\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        test_result+= trainer.test_minibatch(data)\n",
    "        \n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.3062, Error: 92.19%\n",
      "Minibatch: 500, Loss: 0.1375, Error: 1.56%\n",
      "Minibatch: 1000, Loss: 0.0861, Error: 3.12%\n",
      "Minibatch: 1500, Loss: 0.0680, Error: 3.12%\n",
      "Minibatch: 2000, Loss: 0.0268, Error: 0.00%\n",
      "Minibatch: 2500, Loss: 0.0136, Error: 0.00%\n",
      "Minibatch: 3000, Loss: 0.0298, Error: 1.56%\n",
      "Minibatch: 3500, Loss: 0.1896, Error: 7.81%\n",
      "Minibatch: 4000, Loss: 0.0080, Error: 0.00%\n",
      "Minibatch: 4500, Loss: 0.0551, Error: 3.12%\n",
      "Minibatch: 5000, Loss: 0.0121, Error: 0.00%\n",
      "Minibatch: 5500, Loss: 0.0330, Error: 1.56%\n",
      "Minibatch: 6000, Loss: 0.0059, Error: 0.00%\n",
      "Minibatch: 6500, Loss: 0.0208, Error: 0.00%\n",
      "Minibatch: 7000, Loss: 0.0902, Error: 3.12%\n",
      "Minibatch: 7500, Loss: 0.0088, Error: 0.00%\n",
      "Minibatch: 8000, Loss: 0.0204, Error: 1.56%\n",
      "Minibatch: 8500, Loss: 0.0118, Error: 0.00%\n",
      "Minibatch: 9000, Loss: 0.0312, Error: 1.56%\n",
      "Training took 162.8 s\n",
      "Average test error: 1.39%\n"
     ]
    }
   ],
   "source": [
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model(x)\n",
    "    reader_train = create_reader(train_file, True)\n",
    "    reader_test = create_reader(test_file, False)\n",
    "    train_test(reader_train, reader_test, z)\n",
    "    \n",
    "do_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4]\n",
      "[7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "out=C.softmax(z)\n",
    "reader_eval = create_reader(test_file, False)\n",
    "\n",
    "eval_minibatch_size = 25\n",
    "eval_input_map ={x: reader_eval.streams.features,\n",
    "                 y: reader_eval.streams.labels} \n",
    "\n",
    "data = reader_eval.next_minibatch(eval_minibatch_size, input_map = eval_input_map)\n",
    "\n",
    "img_label = data[y].asarray()\n",
    "\n",
    "img_data = data[x].asarray()\n",
    "img_data=np.reshape(img_data,(eval_minibatch_size,1,28,28))\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]\n",
    "\n",
    "\n",
    "pred=[np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "actual_label=[np.argmax(img_label[i]) for i in range(len(img_label))]\n",
    "\n",
    "\n",
    "print(pred[:25])\n",
    "print(actual_label)\n",
    "\n",
    "# i=12\n",
    "# plt.imshow(np.reshape(img_data[12],(28,28)),cmap=\"gray_r\")\n",
    "# print(pred[i])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAA9VJREFUeJzt3UtO41AURdEYMf8pm07oQbB48ed5\nr9UuiapQW7dxnGRZ1/UB9Hyc/RcAziF+iBI/RIkfosQPUeKHKPFDlPghSvwQ9Xnwz/M4Iexv2fKH\nXH6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQP\nUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UPU0V/RzQ+WZdM3Kuesq29035PLD1HihyjxQ5T4\nIUr8ECV+iBI/RNn5D2DH/5/R181zAq+5/BAlfogSP0SJH6LED1HihyjxQ5Sd/w3s+Nc08nspPCPg\n8kOU+CFK/BAlfogSP0SJH6JMfRvNOuftPVnN+rrg8kOW+CFK/BAlfogSP0SJH6LED1F2/hs48+2n\nIz/bMwLncvkhSvwQJX6IEj9EiR+ixA9R4ocoO//TlTfnu36M9Oi/68q/sxm4/BAlfogSP0SJH6LE\nD1HihyjxQ5Sdn8uy4+/L5Yco8UOU+CFK/BAlfogSP0SJH6Ls/JzGjn8ulx+ixA9R4oco8UOU+CFK\n/BBl6nvyVdP78Npcl8sPUeKHKPFDlPghSvwQJX6IEj9E2fnf4K5fob3FrDt++Xf2zeWHKPFDlPgh\nSvwQJX6IEj9EiR+i7Py8NOuO/3jY8v/i8kOU+CFK/BAlfogSP0SJH6LED1F2/pubeaf/ix1/jMsP\nUeKHKPFDlPghSvwQJX6IEj9E2fkncOet/hU7/r5cfogSP0SJH6LED1HihyjxQ5Sp7w2qU9woU965\nXH6IEj9EiR+ixA9R4oco8UOU+CHKzv9kq/8fW/28XH6IEj9EiR+ixA9R4oco8UOU+CHKzh9np+9y\n+SFK/BAlfogSP0SJH6LED1Hihyg7/wRs8ezB5Yco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4\nIUr8ECV+iBI/RHlL7wRGvj7c24H5jcsPUeKHKPFDlPghSvwQJX6IEj9ELQfvwNOOziNbO/dz8ecn\nNv1ndfkhSvwQJX6IEj9EiR+ixA9R4oco7+ff6NWu6xkAZuTyQ5T4IUr8ECV+iBI/RIkfosQPUXb+\nNxh9b7fnBDiDyw9R4oco8UOU+CFK/BAlfogy9V3AxT8Gmpty+SFK/BAlfogSP0SJH6LED1Hihyjx\nQ5T4IUr8ECV+iBI/RIkfosQPUeKHqKPfz+8zquEiXH6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogS\nP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPgh6gsUYk8jkiUFKQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75b37b7278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path=os.path.join(\"MysteryNumberD.png\")\n",
    "image=misc.imread(path,flatten=0)\n",
    "image=image.reshape(1,28,28)\n",
    "ans=out.eval(image.astype(np.float32))\n",
    "print(np.argmax(ans))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.imshow(image.reshape(28,28),cmap='gray_r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
